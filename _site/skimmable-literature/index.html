<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Skimmable Literature - Sami’s Projects &amp; Notes</title>
<meta name="description" content="A compilation of my projects &amp; notes.">


  <meta name="author" content="Sami Kamal">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Sami's Projects & Notes">
<meta property="og:title" content="Skimmable Literature">
<meta property="og:url" content="http://localhost:4000/skimmable-literature/">


  <meta property="og:description" content="A compilation of my projects &amp; notes.">



  <meta property="og:image" content="http://localhost:4000/images/skimmable-literature-images/skimmable-literature-banner.png">









  

  


<link rel="canonical" href="http://localhost:4000/skimmable-literature/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Sami Kamal",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Sami's Projects & Notes Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Sami's Projects & Notes
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/projects">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/coursework">Coursework</a>
            </li><li class="masthead__menu-item">
              <a href="/Resume.pdf">Résumé</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: url('/images/skimmable-literature-images/skimmable-literature-banner.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Skimmable Literature

        
      </h1>
      
      


      
      
    </div>
  
  
</div>







<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/images/smaglantis-images/PlayerHead.gif" alt="Sami Kamal" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Sami Kamal</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>My notes on math, computer science, and cognitive science.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/samikamal21" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Skimmable Literature">
    
    
    

    <div class="page__inner-wrap">
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Overview</h4></header>
              <ul class="toc__menu"><li><a href="#skimmable-literature">Skimmable Literature</a><ul><li><a href="#get-data">Get data</a></li><li><a href="#preprocess-data">Preprocess data</a><ul><li><a href="#get-list-of-sentences">Get list of sentences</a></li></ul></li><li><a href="#make-numeric-labels-ml-models-require-numeric-labels">Make numeric labels (ML models require numeric labels)</a><ul><li><a href="#label-encode-labels">Label encode labels</a></li></ul></li><li><a href="#starting-a-series-of-modelling-experiments">Starting a series of modelling experiments</a></li><li><a href="#model-0-getting-a-baseline">Model 0: Getting a baseline</a><ul><li><a href="#download-the-helper-function-script">Download the helper function script</a></li></ul></li><li><a href="#preparing-our-data-the-text-for-deep-sequence-models">Preparing our data (the text) for deep sequence models</a><ul><li><a href="#create-a-text-vectorizer-layer">Create a text vectorizer layer</a></li><li><a href="#create-custom-text-embedding">Create custom text embedding</a></li></ul></li><li><a href="#creating-datasets-making-sure-our-data-loads-as-fast-as-possible">Creating datasets (making sure our data loads as fast as possible)</a></li><li><a href="#model-1-conv1d-with-token-embeddings">Model 1: Conv1D with token embeddings</a><ul><li><a href="#model-1-results">Model 1 results</a></li></ul></li><li><a href="#model-2-feature-extraction-with-pre-trained-token-embeddings">Model 2: Feature extraction with pre-trained token embeddings</a><ul><li><a href="#building-and-fitting-an-nlp-feature-extraction-model-using-pre-trained-embeddings-from-tensorflow-hub">Building and fitting an NLP feature extraction model using pre-trained embeddings from TensorFlow Hub</a></li><li><a href="#model-2-results">Model 2 results</a></li></ul></li><li><a href="#model-3-conv1d-with-character-embeddings">Model 3: Conv1D with character embeddings</a><ul><li><a href="#creating-a-character-level-tokenizer">Creating a character-level tokenizer</a></li><li><a href="#creating-a-character-level-embedding">Creating a character-level embedding</a></li><li><a href="#building-a-conv1d-model-to-fit-on-character-embeddings">Building a Conv1D model to fit on character embeddings</a></li><li><a href="#model-3-results">Model 3 results</a></li></ul></li><li><a href="#model-4-combining-pretrained-token-embeddings--characters-embeddings-hybrid-embedding-layer">Model 4: Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)</a><ul><li><a href="#combining-token-and-character-data-into-a-tfdata-dataset">Combining token and character data into a tf.data Dataset</a></li><li><a href="#fitting-a-model-on-token-and-character-level-sequences">Fitting a model on token and character-level sequences</a></li><li><a href="#model-4-results">Model 4 results</a></li></ul></li><li><a href="#model-5-transfer-learning-with-pretrained-token-embeddings--character-embeddings--positional-embeddings">Model 5: Transfer learning with pretrained token embeddings + character embeddings + positional embeddings</a><ul><li><a href="#create-positional-embeddings">Create positional embeddings</a></li><li><a href="#building-a-tribrid-embedding-model">Building a tribrid embedding model</a></li><li><a href="#create-tribrid-embedding-datasets-using-tfdata">Create tribrid embedding datasets using tf.data</a></li><li><a href="#fitting-evaluating-and-making-predictions-with-our-tribrid-model">Fitting, evaluating, and making predictions with our tribrid model</a></li><li><a href="#model-5-results">Model 5 results</a></li></ul></li><li><a href="#compare-model-results">Compare model results</a></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#the-full-code">The Full Code</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <style type="text/css">
body {
  font-size: 13pt;
}
</style>

<h1 id="skimmable-literature">Skimmable Literature</h1>

<p>The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.</p>

<p>The paper we’re replicating (the source of the dataset that we’ll be using) is available <a href="https://arxiv.org/abs/1710.06071">here</a>.</p>

<p>And reading through the paper above, we see that the model architecture that they use to achieve their best results is available <a href="https://arxiv.org/abs/1612.05251">here</a>.</p>

<h2 id="get-data">Get data</h2>
<p>Since we’ll be replicating the paper above (PubMed 200k RCT), let’s download the dataset they used.</p>

<p>We can do so from the author’s <a href="https://github.com/Franck-Dernoncourt/pubmed-rct">GitHub</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!git clone https://github.com/Franck-Dernoncourt/pubmed-rct
!ls pubmed-rct
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cloning into 'pubmed-rct'...
remote: Enumerating objects: 39, done.
remote: Counting objects: 100% (14/14), done.
remote: Compressing objects: 100% (9/9), done.
remote: Total 39 (delta 8), reused 5 (delta 5), pack-reused 25
Receiving objects: 100% (39/39), 177.08 MiB | 24.31 MiB/s, done.
Resolving deltas: 100% (15/15), done.
Updating files: 100% (13/13), done.
PubMed_200k_RCT				       PubMed_20k_RCT_numbers_replaced_with_at_sign
PubMed_200k_RCT_numbers_replaced_with_at_sign  README.md
PubMed_20k_RCT
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Start our experiments using the 20k datset with numbers replaced by "@" sign.
</span><span class="n">data_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/</span><span class="sh">"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check all of the filenames in the target directory
</span><span class="kn">import</span> <span class="n">os</span>
<span class="n">filenames</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_dir</span> <span class="o">+</span> <span class="n">filename</span> <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)]</span>
<span class="n">filenames</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',
 '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',
 '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt']
</code></pre></div></div>

<h2 id="preprocess-data">Preprocess data</h2>

<p>Now we’ve got some text data, we should visualize it, so we can understand the dataset more</p>

<p>So, we’ll write a function to read all of the lines of a target text file</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create function to read the lines of a document
</span><span class="k">def</span> <span class="nf">get_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">
  Reads filename (a text filename) and returns the lines of text as a list.

  Args:
    filename: a string containing the target filepath.

  Returns:
    A list of strings with one string per line from the target filename.
  </span><span class="sh">"""</span>
  <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()</span> <span class="c1"># reads remaning lines from the file object and returns them as a list
</span>
<span class="c1"># Let's read in the training lines
</span><span class="n">train_lines</span> <span class="o">=</span> <span class="nf">get_lines</span><span class="p">(</span><span class="n">data_dir</span><span class="o">+</span><span class="sh">"</span><span class="s">train.txt</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># read the lines within the training file
</span><span class="n">train_lines</span><span class="p">[:</span><span class="mi">27</span><span class="p">]</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'###24293578\n',
 'OBJECTIVE\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\n',
 'METHODS\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\n',
 'METHODS\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\n',
 'METHODS\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\n',
 'METHODS\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\n',
 'METHODS\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\n',
 'RESULTS\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\n',
 'RESULTS\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .\n',
 'RESULTS\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\n',
 'RESULTS\tThese differences remained significant at @ weeks .\n',
 'RESULTS\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ) .\n',
 'CONCLUSIONS\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\n',
 '\n',
 '###24854809\n',
 'BACKGROUND\tEmotional eating is associated with overeating and the development of obesity .\n',
 'BACKGROUND\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\n',
 'OBJECTIVE\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\n',
 'OBJECTIVE\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\n',
 'METHODS\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\n',
 'METHODS\tAttentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .\n',
 'METHODS\tSelf-reported emotional eating was assessed with the Dutch Eating Behavior Questionnaire ( DEBQ ) and ad libitum food intake was tested by a disguised food offer .\n',
 'RESULTS\tHierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .\n',
 'RESULTS\tYet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .\n',
 'CONCLUSIONS\tThe current findings show that self-reported emotional eating ( based on the DEBQ ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .\n',
 'CONCLUSIONS\tResults further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .\n',
</code></pre></div></div>

<p>Let’s think about how we want our data to look.</p>

<p>How I think our data would be best represented:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'line_number': 0,
   'target': 'BACKGROUND',
   'text': 'Emotional eating is associated with overeating and the development of obesity .\n'
   'total_lines: 11}
   ...]
</code></pre></div></div>

<p>Let’s write a function that turns each of our datasets into the above format so we can continue to prepare our data for modelling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">
  Returns a list of dictionaries of abstract line data.

  Takes in filename, reads it contents and sorts through each line,
  extracting things like the target label, the text of the sentence,
  how many sentences are in the current abstract and what sentence
  number the target line is.
  </span><span class="sh">"""</span>
  <span class="n">input_lines</span> <span class="o">=</span> <span class="nf">get_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="c1"># get all lines from filename
</span>  <span class="n">abstract_lines</span> <span class="o">=</span> <span class="sh">""</span> <span class="c1"># create an empty abstract
</span>  <span class="n">abstract_samples</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># create an empty list of abstracts
</span>
  <span class="c1"># Loop through each line in the target file
</span>  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">input_lines</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">line</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">"</span><span class="s">###</span><span class="sh">"</span><span class="p">):</span> <span class="c1"># check to see if the line starts with an ID number
</span>      <span class="n">abstract_id</span> <span class="o">=</span> <span class="n">line</span>
      <span class="n">abstract_lines</span> <span class="o">=</span> <span class="sh">""</span> <span class="c1"># reset the abstract string if the line is an ID line since
</span>      <span class="c1"># the else statement below adds lines and we don't want to mix different abstracts
</span>
    <span class="k">elif</span> <span class="n">line</span><span class="p">.</span><span class="nf">isspace</span><span class="p">():</span> <span class="c1"># check to see if line is a new line (\n in this case)
</span>      <span class="n">abstract_line_split</span> <span class="o">=</span> <span class="n">abstract_lines</span><span class="p">.</span><span class="nf">splitlines</span><span class="p">()</span> <span class="c1"># split abstract into seperate lines since they all end in \n
</span>
      <span class="c1"># Iterate through each line in a single abstract and count them at the same time
</span>      <span class="k">for</span> <span class="n">abstract_line_number</span><span class="p">,</span> <span class="n">abstract_line</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">abstract_line_split</span><span class="p">):</span>
        <span class="n">line_data</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># create an empty dictionary for each line
</span>        <span class="n">target_text_split</span> <span class="o">=</span> <span class="n">abstract_line</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># split target label from text
</span>        <span class="n">line_data</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_text_split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get target label
</span>        <span class="n">line_data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_text_split</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">lower</span><span class="p">()</span> <span class="c1"># get target text and lower all uppercase letters
</span>        <span class="n">line_data</span><span class="p">[</span><span class="sh">"</span><span class="s">line_number</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">abstract_line_number</span> <span class="c1"># what number line does the line appear in the abstract
</span>        <span class="n">line_data</span><span class="p">[</span><span class="sh">"</span><span class="s">total_lines</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">abstract_line_split</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># how many total lines are there in the target abstract? (start from 0 so we -1)
</span>        <span class="n">abstract_samples</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">line_data</span><span class="p">)</span> <span class="c1"># add line data to abstract samples list
</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># if line does not start with # (ID) or \n (new line), all lines between the 2 are part of the same abstract
</span>      <span class="n">abstract_lines</span> <span class="o">+=</span> <span class="n">line</span>

  <span class="k">return</span> <span class="n">abstract_samples</span>
</code></pre></div></div>

<p>Here is what 1 iteration would look like with the sample data:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'###24854809\n',
'BACKGROUND\tEmotional eating is associated with overeating and the development of obesity .\n',
...
'CONCLUSIONS\tResults further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .\n',
'\n',
</code></pre></div></div>

<p>Steps:</p>
<ol>
  <li>Start of Abstract:
    <ul>
      <li>The function reads the first line ‘###24854809\n’. This line starts with “###”, signaling the start of a new abstract. The function stores this ID, although it’s not used in the final output according to your desired format.</li>
    </ul>
  </li>
  <li>Reading Lines Within the Abstract:
    <ul>
      <li>The function then reads the next lines, which are part of the abstract. Let’s take the first content line as an example:
 ‘BACKGROUND\tEmotional eating is associated with overeating and the development of obesity .\n’</li>
    </ul>
  </li>
  <li>Processing the Line:
    <ul>
      <li>The function does not encounter a new “###” or a whitespace line, so it appends this line to <code class="language-plaintext highlighter-rouge">abstract_lines</code>, a variable accumulating the lines of the current abstract.</li>
    </ul>
  </li>
  <li>End of Abstract:
    <ul>
      <li>Once the function reads the empty line ‘\n’, it recognizes the end of the current abstract.</li>
    </ul>
  </li>
  <li>Splitting Abstract into Lines:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">abstract_lines</code> is split into separate lines using splitlines(), resulting in individual lines for processing.</li>
    </ul>
  </li>
  <li>
    <p>Extracting Information from Each Line:
 *For each line in the split abstract, the function:
     Splits the line into the category and text (separated by a tab character \t). For our example line, <code class="language-plaintext highlighter-rouge">target = 'BACKGROUND' and text = 'Emotional eating is associated with overeating and the development of obesity .'</code>
     * Lowers the case of the text.
     * Counts the line number within the abstract.
     * Calculates the total number of lines in the abstract.</p>
  </li>
  <li>Creating the Dictionary:
    <ul>
      <li>For the example line, the function creates a dictionary:</li>
    </ul>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> {
 'line_number': 0, 
 'target': 'BACKGROUND', 
 'text': 'Emotional eating is associated with overeating and the development of obesity .', 
 'total_lines': 11
 }
</code></pre></div>    </div>
    <ul>
      <li>This is repeated for each line in the abstract</li>
    </ul>
  </li>
  <li>Appending to the Output List:
    <ul>
      <li>Each line’s dictionary is added to <code class="language-plaintext highlighter-rouge">abstract_samples</code>, a list that accumulates the dictionaries.</li>
    </ul>
  </li>
  <li>Continuing the Process:
    <ul>
      <li>The function then continues this process for each subsequent line and for each new abstract it encounters in the file.</li>
    </ul>
  </li>
</ol>

<p>Finally, the function returns <code class="language-plaintext highlighter-rouge">abstract_samples</code>, a list of dictionaries where each dictionary represents a line from the abstracts.</p>

<p>Now, let’s this function in action.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get data from file and preprocess it
</span><span class="n">train_samples</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="sh">"</span><span class="s">train.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">val_samples</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="sh">"</span><span class="s">dev.txt</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># dev is another name for valiation data set
</span><span class="n">test_samples</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="sh">"</span><span class="s">test.txt</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Check the first abstract of our training data
</span><span class="n">train_samples</span><span class="p">[:</span><span class="mi">14</span><span class="p">]</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'target': 'OBJECTIVE',
  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',
  'line_number': 0,
  'total_lines': 11},
 {'target': 'METHODS',
  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',
  'line_number': 1,
  'total_lines': 11},
 {'target': 'METHODS',
  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',
  'line_number': 2,
  'total_lines': 11},
 {'target': 'METHODS',
  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',
  'line_number': 3,
  'total_lines': 11},
 {'target': 'METHODS',
  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',
  'line_number': 4,
  'total_lines': 11},
 {'target': 'METHODS',
  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',
  'line_number': 5,
  'total_lines': 11},
 {'target': 'RESULTS',
  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',
  'line_number': 6,
  'total_lines': 11},
 {'target': 'RESULTS',
  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .',
  'line_number': 7,
  'total_lines': 11},
 {'target': 'RESULTS',
  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',
  'line_number': 8,
  'total_lines': 11},
 {'target': 'RESULTS',
  'text': 'these differences remained significant at @ weeks .',
  'line_number': 9,
  'total_lines': 11},
 {'target': 'RESULTS',
  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ) .',
  'line_number': 10,
  'total_lines': 11},
 {'target': 'CONCLUSIONS',
  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',
  'line_number': 11,
  'total_lines': 11},
 {'target': 'BACKGROUND',
  'text': 'emotional eating is associated with overeating and the development of obesity .',
  'line_number': 0,
  'total_lines': 10},
 {'target': 'BACKGROUND',
  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',
  'line_number': 1,
  'total_lines': 10}]
</code></pre></div></div>

<p>Now that our data is in the format of a list of dictionaries, let’s turn it into a DataFrame to further visualize it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">train_samples</span><span class="p">)</span>
<span class="n">val_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">val_samples</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">test_samples</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/skim-df.png" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Distribution of labels in training data
</span><span class="n">train_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>METHODS        59353
RESULTS        57953
CONCLUSIONS    27168
BACKGROUND     21727
OBJECTIVE      13839
Name: target, dtype: int64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's check the length of different lines
</span><span class="n">train_df</span><span class="p">.</span><span class="n">total_lines</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="nf">hist</span><span class="p">();</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/skim-graph.png" alt="" /></p>

<h3 id="get-list-of-sentences">Get list of sentences</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert abstract text lines into lists
</span><span class="n">train_sentences</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
<span class="n">val_sentences</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
<span class="n">test_sentences</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">len</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_sentences</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_sentences</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: (180040, 30212, 30135)</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># View the 10 lines of training sentences
</span><span class="n">train_sentences</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',
 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',
 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',
 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',
 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',
 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',
 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',
 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .',
 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',
 'these differences remained significant at @ weeks .']
</code></pre></div></div>

<h2 id="make-numeric-labels-ml-models-require-numeric-labels">Make numeric labels (ML models require numeric labels)</h2>

<p>We’ll see the following 2 ways on how we can make our labels numeric:</p>
<ul>
  <li>One hot encode labels</li>
  <li>Label encode labels</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># One hot encode labels
</span><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># we want a non-sparse matrix since TensorFlow is incompatiable with the sparse format
</span><span class="n">train_labels_one_hot</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># use -1 to count # of elements in target
</span><span class="n">val_labels_one_hot</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">val_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># always use transform for validation data to avoid data leakage from train data
</span><span class="n">test_labels_one_hot</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Check what one hot encoded labels look like
</span><span class="n">train_labels_one_hot</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0., 0., 0., 1., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 1., 0., 0.],
       ...,
       [0., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0.]])
</code></pre></div></div>

<h3 id="label-encode-labels">Label encode labels</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract labels ("target" columns) and encode them into integers
</span><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="nc">LabelEncoder</span><span class="p">()</span>
<span class="n">train_labels_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">())</span>
<span class="n">val_labels_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">val_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">())</span>
<span class="n">test_labels_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">())</span>

<span class="c1"># Check what training labels look like
</span><span class="n">train_labels_encoded</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: array([3, 2, 2, ..., 4, 1, 1])</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get class name and number of classes from LabelEncoder instance
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">label_encoder</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="n">classes_</span>
<span class="n">num_classes</span><span class="p">,</span> <span class="n">class_names</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5,
 array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],
       dtype=object))
</code></pre></div></div>

<h2 id="starting-a-series-of-modelling-experiments">Starting a series of modelling experiments</h2>

<p>We’re going to be trying out a bunch of different models and seeing which one works best.</p>

<p>As always, we’re going to start with a baseline (TF-IDF Multinomial Naive Bayes Classifier).</p>

<h2 id="model-0-getting-a-baseline">Model 0: Getting a baseline</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Create a pipeline
</span><span class="n">model_0</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">tf-idf</span><span class="sh">"</span><span class="p">,</span> <span class="nc">TfidfVectorizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">clf</span><span class="sh">"</span><span class="p">,</span> <span class="nc">MultinomialNB</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Fit the pipeline to the training data
</span><span class="n">model_0</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_sentences</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">train_labels_encoded</span><span class="p">)</span>

<span class="c1"># Evaluate baseline model on validation data
</span><span class="n">model_0</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">val_sentences</span><span class="p">,</span>
              <span class="n">y</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: 0.7218323844829869</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make predicions using baseline model
</span><span class="n">baseline_preds</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">val_sentences</span><span class="p">)</span>
<span class="n">baseline_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: array([4, 1, 3, ..., 4, 4, 1])</code></p>

<h3 id="download-the-helper-function-script">Download the helper function script</h3>

<p>We will import <a href="https://github.com/samikamal21/Helper-Functions/edit/main/helper_functions.py">helper function script</a> which contains a useful method to evaluate our model on different metrics.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!wget https://raw.githubusercontent.com/samikamal21/Helper-Functions/main/helper_functions.py
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">helper_functions</span> <span class="kn">import</span> <span class="n">calculate_results</span>

<span class="c1"># Calculate baseline results
</span><span class="n">baseline_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                     <span class="n">y_pred</span><span class="o">=</span><span class="n">baseline_preds</span><span class="p">)</span>
<span class="n">baseline_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.1832384482987,
 'precision': 71.86466952323352,
 'recall': 72.1832384482987,
 'f1': 69.89250353450294}
</code></pre></div></div>

<p>Not bad metrics for our baseline. Hopefully, we can improve these metrics as we continue along our experiments.</p>

<h2 id="preparing-our-data-the-text-for-deep-sequence-models">Preparing our data (the text) for deep sequence models</h2>

<p>Before we start building deeper models, we’ve got to create vectorization and embedding layers.</p>

<ul>
  <li>
    <p><strong>Vectorization:</strong> Turning text or images into numbers so computers can understand and process them. It’s like translating languages where words or pixels become numbers.</p>
  </li>
  <li>
    <p><strong>Embedding Layers:</strong> A special layer in deep learning that learns a smarter way to turn words or items into numbers, capturing more about what they mean or how they relate to each other. It’s like creating a secret code where similar things have similar codes.</p>
  </li>
</ul>

<p>We should first find out on average, how long our sentences are before we implement the layers since we will need to know for the vectorization layer.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># How long is each sentence on average?
</span><span class="n">sent_lens</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">train_sentences</span><span class="p">]</span>
<span class="n">avg_sent_len</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sent_lens</span><span class="p">)</span>
<span class="n">avg_sent_len</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: 26.338269273494777</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># What's the distribution look like?
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">sent_lens</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/distribution.png" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># How long of a sentence length covers 95% of examples?
</span><span class="n">output_seq_len</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">sent_lens</span><span class="p">,</span> <span class="mi">95</span><span class="p">))</span>
<span class="n">output_seq_len</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: 55</code></p>

<h3 id="create-a-text-vectorizer-layer">Create a text vectorizer layer</h3>

<p>We want to make a layer that maps our texts from words to numbers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.layers.experimental.preprocessing</span> <span class="kn">import</span> <span class="n">TextVectorization</span>

<span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">68000</span> <span class="c1"># max number of words to have in our vocabulary (taken from table 2 in: https://arxiv.org/pdf/1710.06071.pdf)
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">55</span> <span class="c1"># max length our sequences will be
</span>
<span class="n">text_vectorizer</span> <span class="o">=</span> <span class="nc">TextVectorization</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span> <span class="c1"># number of words in vocabulary
</span>                                    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span> <span class="c1"># desired output length of vectorized sequences
</span>
<span class="c1"># Adapt text vectorizer to training sentences
</span><span class="n">text_vectorizer</span><span class="p">.</span><span class="nf">adapt</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">)</span>

<span class="c1"># Test out text vectorizer on random sentences
</span><span class="kn">import</span> <span class="n">random</span>
<span class="n">target_sentence</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Text:</span><span class="se">\n</span><span class="si">{</span><span class="n">target_sentence</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Length of text: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">target_sentence</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vectorized text: </span><span class="si">{</span><span class="nf">text_vectorizer</span><span class="p">([</span><span class="n">target_sentence</span><span class="p">])</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text:
atherosclerosis is a postprandial phenomenon .

Length of text: 6

Vectorized text: [[2813   20    8 1241 6161    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># How many words in our training vocabulary
</span><span class="n">rct_20k_text_vocab</span> <span class="o">=</span> <span class="n">text_vectorizer</span><span class="p">.</span><span class="nf">get_vocabulary</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of words in the vocab: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">rct_20k_text_vocab</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Most common words in the vocab: </span><span class="si">{</span><span class="n">rct_20k_text_vocab</span><span class="p">[</span><span class="si">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Least common words in the vocab: </span><span class="si">{</span><span class="n">rct_20k_text_vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="si">:</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of words in the vocab: 64841
Most common words in the vocab: ['', '[UNK]', 'the', 'and', 'of']
Least common words in the vocab: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']
</code></pre></div></div>

<h3 id="create-custom-text-embedding">Create custom text embedding</h3>

<p>An embedding layer can help our model to learn how words are related to each other.</p>

<p>Essentially, an embedding layer converts words into vectors of real numbers. These vectors represent the words in a continuous vector space where semantically similar words are mapped to nearby points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">token_embed</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">rct_20k_text_vocab</span><span class="p">),</span> <span class="c1"># length of our vocabulary
</span>                               <span class="n">output_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="c1"># Note: different embedding sizes result in drastically different numbers of parameters to train
</span>                               <span class="n">mask_zero</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># use masking to handle variable sequence lengths (save space), so the model during training will ignore the 0's to be more computationally efficient
</span>                               <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">token_embedding</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p>Let’s view an example using the <code class="language-plaintext highlighter-rouge">token_embed</code> layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show example embedding
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence before vectorization:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">target_sentence</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="n">vectorized_sentence</span> <span class="o">=</span> <span class="nf">text_vectorizer</span><span class="p">([</span><span class="n">target_sentence</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence after vectorization (before embedding):</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">vectorized_sentence</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embedded_sentence</span> <span class="o">=</span> <span class="nf">token_embed</span><span class="p">(</span><span class="n">vectorized_sentence</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence after embedding:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">embedded_sentence</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Embedded sentence shape: </span><span class="si">{</span><span class="n">embedded_sentence</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sentence before vectorization:
 atherosclerosis is a postprandial phenomenon .

Sentence after vectorization (before embedding):
 [[2813   20    8 1241 6161    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0]] 

Sentence after embedding:
 [[[ 0.02908354 -0.02426955 -0.03805091 ...  0.00903618  0.04704112
   -0.03753959]
  [-0.04828575 -0.01488106 -0.03769809 ... -0.03505546  0.04465925
    0.01680866]
  [ 0.03425646  0.03925902  0.0173355  ... -0.01982297 -0.01273812
    0.02844638]
  ...
  [-0.00058524 -0.03278247 -0.00265789 ... -0.02257251  0.04044208
   -0.04219208]
  [-0.00058524 -0.03278247 -0.00265789 ... -0.02257251  0.04044208
   -0.04219208]
  [-0.00058524 -0.03278247 -0.00265789 ... -0.02257251  0.04044208
   -0.04219208]]]

Embedded sentence shape: (1, 55, 128)
</code></pre></div></div>

<h2 id="creating-datasets-making-sure-our-data-loads-as-fast-as-possible">Creating datasets (making sure our data loads as fast as possible)</h2>

<p>We’re going to set up our data to run as fast as possible with the TensorFlow tf.data API.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Turn our data into TensorFlow Datasets
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">train_sentences</span><span class="p">,</span> <span class="n">train_labels_one_hot</span><span class="p">))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">val_sentences</span><span class="p">,</span> <span class="n">val_labels_one_hot</span><span class="p">))</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">test_sentences</span><span class="p">,</span> <span class="n">test_labels_one_hot</span><span class="p">))</span>

<span class="n">train_dataset</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">Output: &lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))&gt;</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Take the TensorSliceDatasets and turn them into prefetched datasets which will speed up training
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span> <span class="c1"># batch data into size 32 and prefetch it with AUTOTUNE, which will automatically prefetch as many samples possible
</span><span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="n">train_dataset</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;</code></p>

<h2 id="model-1-conv1d-with-token-embeddings">Model 1: Conv1D with token embeddings</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create 1D conv model to process sequences
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>
<span class="n">text_vectors</span> <span class="o">=</span> <span class="nf">text_vectorizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># vectorize text inputs
</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="nf">token_embed</span><span class="p">(</span><span class="n">text_vectors</span><span class="p">)</span> <span class="c1"># create embedding
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">token_embeddings</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">GlobalAveragePooling1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># condense the output of our feature vector from conv layer since we need it to be 1d
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">model_1</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="n">model_1</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVe  (None, 55)                0         
 ctorization)                                                    
                                                                 
 token_embedding (Embedding  (None, 55, 128)           8299648   
 )                                                               
                                                                 
 conv1d (Conv1D)             (None, 55, 64)            41024     
                                                                 
 global_average_pooling1d (  (None, 64)                0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense (Dense)               (None, 5)                 325       
                                                                 
=================================================================
Total params: 8340997 (31.82 MB)
Trainable params: 8340997 (31.82 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model
</span><span class="n">history_model_1</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                              <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)),</span> <span class="c1"># we'll only look at 10% of the batches to speed up training
</span>                              <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                              <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">),</span>
                              <span class="n">validation_steps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)))</span> <span class="c1"># only validate on 10% of batches
</span>
<span class="c1"># Evaluate on whole validation dataset
</span><span class="n">model_1</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>945/945 [==============================] - 4s 4ms/step - loss: 0.7845 - accuracy: 0.6950
[0.7844608426094055, 0.6949887275695801]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make predictions (our model predicts prediction probabilites for each class)
</span><span class="n">model_1_pred_probs</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>

<span class="c1"># Convert pred probs to classes
</span><span class="n">model_1_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">model_1_pred_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># check cols for highest probability
</span><span class="n">model_1_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 0, ..., 4, 4, 0])&gt;</code></p>

<h3 id="model-1-results">Model 1 results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate model_1 results
</span><span class="n">model_1_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="o">=</span><span class="n">model_1_preds</span><span class="p">)</span>
<span class="n">model_1_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 69.49887461935654,
 'precision': 67.57688830597704,
 'recall': 69.49887461935654,
 'f1': 66.74301745230301}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print baseline_results
</span><span class="n">baseline_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.1832384482987,
 'precision': 71.86466952323352,
 'recall': 72.1832384482987,
 'f1': 69.89250353450294}
</code></pre></div></div>

<p>While <code class="language-plaintext highlighter-rouge">model_1</code> didn’t beat our baseline results, that’s okay since we’ll build more models and utilize transfer learning.</p>

<h2 id="model-2-feature-extraction-with-pre-trained-token-embeddings">Model 2: Feature extraction with pre-trained token embeddings</h2>

<p>Now let’s use pre-trained word embeddings from TensorFlow Hub, more specifically the universal <a href="https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/tensorFlow2/variations/universal-sentence-encoder/versions/2?tfhub-redirect=true">sentence encoder (USE)</a>.</p>

<p>The paper originally used GloVe embeddings, however, we’re going to stick with the later-created USE pre-trained embeddings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Download pretrained TensorFlow Hub USE
</span><span class="kn">import</span> <span class="n">tensorflow_hub</span> <span class="k">as</span> <span class="n">hub</span>
<span class="n">tf_hub_embedding_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="nc">KerasLayer</span><span class="p">(</span><span class="sh">"</span><span class="s">https://tfhub.dev/google/universal-sentence-encoder/4</span><span class="sh">"</span><span class="p">,</span>
                                        <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">universal_sentence_encoder</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Test out the pretained embeddings on a random setnence
</span><span class="n">random_train_sentence</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Random Sentences:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">random_train_sentence</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="n">use_embedded_sentence</span> <span class="o">=</span> <span class="nf">tf_hub_embedding_layer</span><span class="p">([</span><span class="n">random_train_sentence</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence after embedding:</span><span class="se">\n</span><span class="si">{</span><span class="n">use_embedded_sentence</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="si">:</span><span class="mi">30</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of sentence embedding: </span><span class="si">{</span><span class="nf">len</span><span class="p">((</span><span class="n">use_embedded_sentence</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random Sentences:
 group-@ received local anaesthetic infiltration along the proposed incision lines and intercostals block before the rib harvest .

Sentence after embedding:
[ 0.01634574  0.01984339 -0.03107073 -0.07532325 -0.05695229 -0.00491949
  0.06840778  0.00047784 -0.06148654 -0.00743734  0.0787323   0.05070838
  0.02248778  0.07248233 -0.0134852   0.00810584 -0.04671064  0.00120152
 -0.07101137  0.07770029 -0.06347459  0.07168395 -0.03889387 -0.02629391
 -0.03117911  0.04108566 -0.0124218  -0.03828406  0.03202581  0.01672265]

Length of sentence embedding: 512
</code></pre></div></div>

<h3 id="building-and-fitting-an-nlp-feature-extraction-model-using-pre-trained-embeddings-from-tensorflow-hub">Building and fitting an NLP feature extraction model using pre-trained embeddings from TensorFlow Hub</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define feature extraction model using TF Hub layer
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">)</span> <span class="c1"># model can take any shape
</span><span class="n">pretrained_embedding</span> <span class="o">=</span> <span class="nf">tf_hub_embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># tokenize text and create embedding of each sequecne (512 long vector)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">pretrained_embedding</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># create output layer of 5 since we have 5 classes/labels
</span><span class="n">model_2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">model_2_USE_feature_extractor</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">model_2</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="n">model_2</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output:</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_2_USE_feature_extractor"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None,)]                 0         
                                                                 
 universal_sentence_encoder  (None, 512)               256797824 
  (KerasLayer)                                                   
                                                                 
 dense_1 (Dense)             (None, 128)               65664     
                                                                 
 dense_2 (Dense)             (None, 5)                 645       
                                                                 
=================================================================
Total params: 256864133 (979.86 MB)
Trainable params: 66309 (259.02 KB)
Non-trainable params: 256797824 (979.61 MB)
_________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model
</span><span class="n">history_model_2</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                              <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)),</span>
                              <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                              <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
                              <span class="n">validation_steps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)))</span>

<span class="c1"># Evaluate on the whole validation dataset
</span><span class="n">model_2</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>45/945 [==============================] - 11s 11ms/step - loss: 0.8295 - accuracy: 0.6802
[0.8295000195503235, 0.6802263855934143]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make predictions with feature extraction model
</span><span class="n">model_2_pred_probs</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>

<span class="c1"># Convert the predictions probabilites with feature extraction model to labels
</span><span class="n">model_2_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">model_2_pred_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_2_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 0, ..., 4, 4, 2])&gt;</code></p>

<h3 id="model-2-results">Model 2 results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate results from TF Hub pretrained embeddings results on val set
</span><span class="n">model_2_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="o">=</span><span class="n">model_2_preds</span><span class="p">)</span>

<span class="n">model_2_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 68.02264001059181,
 'precision': 67.78869921734031,
 'recall': 68.02264001059181,
 'f1': 67.26868729044887}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print baseline_results
</span><span class="n">baseline_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.1832384482987,
 'precision': 71.86466952323352,
 'recall': 72.1832384482987,
 'f1': 69.89250353450294}
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">model_2</code> did not beat our baseline or <code class="language-plaintext highlighter-rouge">model_1</code>, which is good to know for more experimentation as we build more models. We’ll next try a Conv1D model with a character embedding layer and see what effects it has on the model.</p>

<h2 id="model-3-conv1d-with-character-embeddings">Model 3: Conv1D with character embeddings</h2>

<p>The paper that we’re replicating states they used a combination of token and character-level embeddings.</p>

<p>Previously we’ve made token-level embeddings but we’ll need to do similar steps for characters if we want to use char-level embeddings.</p>

<h3 id="creating-a-character-level-tokenizer">Creating a character-level tokenizer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make function to split sentences into characters
</span><span class="k">def</span> <span class="nf">split_chars</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="k">return</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
  <span class="c1"># casting it to a list automically breaks up sentence into a list of characters,
</span>  <span class="c1"># then add a space in between each character which turns it into a string
</span>
<span class="c1"># Test splitting non-character level sequence into characters
</span><span class="nf">split_chars</span><span class="p">(</span><span class="n">random_train_sentence</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g r o u p - @   r e c e i v e d   l o c a l   a n a e s t h e t i c   i n f i l t r a t i o n   a l o n g   t h e   p r o p o s e d   i n c i s i o n   l i n e s   a n d   i n t e r c o s t a l s   b l o c k   b e f o r e   t h e   r i b   h a r v e s t   .
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split sequence-level data splits into character-level data splits
</span><span class="n">train_chars</span> <span class="o">=</span> <span class="p">[</span><span class="nf">split_chars</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">train_sentences</span><span class="p">]</span>
<span class="n">val_chars</span> <span class="o">=</span> <span class="p">[</span><span class="nf">split_chars</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">val_sentences</span><span class="p">]</span>
<span class="n">test_chars</span> <span class="o">=</span> <span class="p">[</span><span class="nf">split_chars</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">]</span>
<span class="n">train_chars</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .',
 'a   t o t a l   o f   @   p a t i e n t s   w i t h   p r i m a r y   k n e e   o a   w e r e   r a n d o m i z e d   @ : @   ;   @   r e c e i v e d   @   m g / d a y   o f   p r e d n i s o l o n e   a n d   @   r e c e i v e d   p l a c e b o   f o r   @   w e e k s   .',
 'o u t c o m e   m e a s u r e s   i n c l u d e d   p a i n   r e d u c t i o n   a n d   i m p r o v e m e n t   i n   f u n c t i o n   s c o r e s   a n d   s y s t e m i c   i n f l a m m a t i o n   m a r k e r s   .',
 'p a i n   w a s   a s s e s s e d   u s i n g   t h e   v i s u a l   a n a l o g   p a i n   s c a l e   (   @ - @   m m   )   .',
 's e c o n d a r y   o u t c o m e   m e a s u r e s   i n c l u d e d   t h e   w e s t e r n   o n t a r i o   a n d   m c m a s t e r   u n i v e r s i t i e s   o s t e o a r t h r i t i s   i n d e x   s c o r e s   ,   p a t i e n t   g l o b a l   a s s e s s m e n t   (   p g a   )   o f   t h e   s e v e r i t y   o f   k n e e   o a   ,   a n d   @ - m i n   w a l k   d i s t a n c e   (   @ m w d   )   .']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># What's the average character length?
</span><span class="n">char_lens</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">train_sentences</span><span class="p">]</span>
<span class="n">mean_char_len</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">char_lens</span><span class="p">)</span>
<span class="n">mean_char_len</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: 149.3662574983337</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check the distribution of our sequences at a character level
</span><span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">char_lens</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/skim-graph2.png" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find what character length covers 95% of sequences
</span><span class="n">output_seq_char_len</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">char_lens</span><span class="p">,</span> <span class="mi">95</span><span class="p">))</span>
<span class="n">output_seq_char_len</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: 290</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get all keyboard characters
</span><span class="kn">import</span> <span class="n">string</span>
<span class="n">alphabet</span> <span class="o">=</span> <span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span> <span class="o">+</span> <span class="n">string</span><span class="p">.</span><span class="n">digits</span> <span class="o">+</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span>
<span class="n">alphabet</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: abcdefghijklmnopqrstuvwxyz0123456789!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_{|}~</code></p>

<p>Now we are ready to create a char-level token vectorizer layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create char-level token vectorizer instance
</span><span class="n">NUM_CHAR_TOKENS</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="c1"># add 2 for space and OOV token (OOV = out of vocab, '[UNK]')
</span><span class="n">char_vectorizer</span> <span class="o">=</span> <span class="nc">TextVectorization</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="n">NUM_CHAR_TOKENS</span><span class="p">,</span>
                                    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">output_seq_char_len</span><span class="p">,</span>
                                    <span class="n">standardize</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># set standardize to "None" if you want to leave punctuation in
</span>                                    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_vectorizer</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Adapt character vectorizer to training characters
</span><span class="n">char_vectorizer</span><span class="p">.</span><span class="nf">adapt</span><span class="p">(</span><span class="n">train_chars</span><span class="p">)</span>

<span class="c1"># Check character vocab stats
</span><span class="n">char_vocab</span> <span class="o">=</span> <span class="n">char_vectorizer</span><span class="p">.</span><span class="nf">get_vocabulary</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of different characters in character vocab: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">char_vocab</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">5 most common characters: </span><span class="si">{</span><span class="n">char_vocab</span><span class="p">[</span><span class="si">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">5 least common characters: </span><span class="si">{</span><span class="n">char_vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="si">:</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of different characters in character vocab: 57
5 most common characters: ['', '[UNK]', 'e', 't', 'i']
5 least common characters: ['|', '"', ']', '\\', '[']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test out character vectorizer
</span><span class="n">random_train_chars</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_chars</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Charified text:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">random_train_chars</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of random_train_chars: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">random_train_chars</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">vectorized_chars</span> <span class="o">=</span> <span class="nf">char_vectorizer</span><span class="p">([</span><span class="n">random_train_chars</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vectorized chars:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">vectorized_chars</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Length of vectorized chars: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">vectorized_chars</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Charified text:
 m e d i a n   u n i t s   o f   i n s u l i n   a t   t h e   e n d   o f   t h e   s t u d y   w a s   @   u   i n   t h e   p l a c e b o   g r o u p   a n d   @   u   i n   t h e   v i l d a g l i p t i n   g r o u p   .

Length of random_train_chars: 88

Vectorized chars:
 [[15  2 10  4  5  6 16  6  4  3  9  7 17  4  6  9 16 12  4  6  5  3  3 13
   2  2  6 10  7 17  3 13  2  9  3 16 10 20 21  5  9 19 16  4  6  3 13  2
  14 12  5 11  2 23  7 18  8  7 16 14  5  6 10 19 16  4  6  3 13  2 22  4
  12 10  5 18 12  4 14  3  4  6 18  8  7 16 14 25  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0]]

Length of vectorized chars: 290
</code></pre></div></div>

<p>Now, we are ready to create a character-level embedding layer for our model.</p>
<h3 id="creating-a-character-level-embedding">Creating a character-level embedding</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create char embedding layer
</span><span class="n">char_embed</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">char_vocab</span><span class="p">),</span> <span class="c1"># number of different characters
</span>                              <span class="n">output_dim</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="c1"># this is the size of the char embedding in the paper
</span>                              <span class="n">mask_zero</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_embed</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Test our character embedding layer
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Charified text:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">random_train_chars</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="n">char_embed_example</span> <span class="o">=</span> <span class="nf">char_embed</span><span class="p">(</span><span class="nf">char_vectorizer</span><span class="p">([</span><span class="n">random_train_chars</span><span class="p">]))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Embedded chars. (after vectorization and embedding):</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">char_embed_example</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Character embedding shape: </span><span class="si">{</span><span class="n">char_embed_example</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Charified text:
 m e d i a n   u n i t s   o f   i n s u l i n   a t   t h e   e n d   o f   t h e   s t u d y   w a s   @   u   i n   t h e   p l a c e b o   g r o u p   a n d   @   u   i n   t h e   v i l d a g l i p t i n   g r o u p   .

Embedded chars. (after vectorization and embedding):
 [[[-0.04383502  0.03714396 -0.03769231 ...  0.04354905  0.01268679
   -0.0142444 ]
  [ 0.03809137 -0.00407938 -0.00563384 ...  0.00669958  0.02575716
    0.03598731]
  [-0.03750778  0.01207554 -0.00258582 ...  0.02083546 -0.03991684
    0.04187199]
  ...
  [-0.04438063  0.03701902 -0.0448746  ... -0.03448347  0.04611773
   -0.03295266]
  [-0.04438063  0.03701902 -0.0448746  ... -0.03448347  0.04611773
   -0.03295266]
  [-0.04438063  0.03701902 -0.0448746  ... -0.03448347  0.04611773
   -0.03295266]]]

Character embedding shape: (1, 290, 25)
</code></pre></div></div>

<h3 id="building-a-conv1d-model-to-fit-on-character-embeddings">Building a Conv1D model to fit on character embeddings</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make Conv1D on chars only
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>
<span class="n">char_vectors</span> <span class="o">=</span> <span class="nf">char_vectorizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">char_embeddings</span> <span class="o">=</span> <span class="nf">char_embed</span><span class="p">(</span><span class="n">char_vectors</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">char_embeddings</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">GlobalMaxPool1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model_3</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">model_3_conv1d_char_embeddings</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">model_3</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Get a summary
</span><span class="n">model_3</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_3_conv1d_char_embeddings"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1)]               0         
                                                                 
 char_vectorizer (TextVecto  (None, 290)               0         
 rization)                                                       
                                                                 
 char_embed (Embedding)      (None, 290, 25)           1425      
                                                                 
 conv1d_1 (Conv1D)           (None, 290, 64)           16064     
                                                                 
 global_max_pooling1d (Glob  (None, 64)                0         
 alMaxPooling1D)                                                 
                                                                 
 dense_3 (Dense)             (None, 128)               8320      
                                                                 
 dense_4 (Dense)             (None, 5)                 645       
                                                                 
=================================================================
Total params: 26454 (103.34 KB)
Trainable params: 26454 (103.34 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<p>Now we need to turn our datasets into char-level datasets to feed into our model. Then, we can fit the model and make some predictions with it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create char level datasets
</span><span class="n">train_char_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">train_chars</span><span class="p">,</span> <span class="n">train_labels_one_hot</span><span class="p">)).</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">val_char_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">val_chars</span><span class="p">,</span> <span class="n">val_labels_one_hot</span><span class="p">)).</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">test_char_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">test_chars</span><span class="p">,</span> <span class="n">test_labels_one_hot</span><span class="p">)).</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Fit the model on chars only
</span><span class="n">history_model_3</span> <span class="o">=</span> <span class="n">model_3</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_char_dataset</span><span class="p">,</span>
                              <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_char_dataset</span><span class="p">)),</span>
                              <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                              <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">validation_data</span><span class="o">=</span><span class="n">val_char_dataset</span><span class="p">,</span>
                              <span class="n">validation_steps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_char_dataset</span><span class="p">)))</span>

<span class="c1"># Make predictions with character model only
</span><span class="n">model_3_pred_probs</span> <span class="o">=</span> <span class="n">model_3</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">val_char_dataset</span><span class="p">)</span>

<span class="c1"># Convert prediction probabilites to class labels
</span><span class="n">model_3_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">model_3_pred_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_3_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 3, ..., 4, 2, 0])&gt;</code></p>

<h3 id="model-3-results">Model 3 results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate results for Conv1D model chars only
</span><span class="n">model_3_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="o">=</span><span class="n">model_3_preds</span><span class="p">)</span>
<span class="n">model_3_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.72275916854231,
 'precision': 72.22725961774881,
 'recall': 72.72275916854231,
 'f1': 71.93359956231276}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print baseline results
</span><span class="n">baseline_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.1832384482987,
 'precision': 71.86466952323352,
 'recall': 72.1832384482987,
 'f1': 69.89250353450294}
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">model_3</code> was barely able to beat our baseline, but it did not improve by much. Maybe we can get better results if we combine our character embedding and token embedding layer.</p>

<h2 id="model-4-combining-pretrained-token-embeddings--characters-embeddings-hybrid-embedding-layer">Model 4: Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)</h2>

<ol>
  <li>
    <p>Create a token-level embedding model (similar to <code class="language-plaintext highlighter-rouge">model_1</code>).</p>
  </li>
  <li>
    <p>Create a character-level model (similar to <code class="language-plaintext highlighter-rouge">model_3</code> with a slight modification).</p>
  </li>
  <li>
    <p>Combine 1 &amp; 2 with a concatenate layer (<code class="language-plaintext highlighter-rouge">layers.Concatenate</code>).</p>
  </li>
  <li>
    <p>Build a series of output layers on top of 3 to Figure 1 and section 4.2 from the <a href="https://arxiv.org/pdf/1612.05251.pdf">paper</a>.</p>
  </li>
  <li>
    <p>Construct a model that takes token and character-level sequences as input and produces sequence label probabilities as output.</p>
  </li>
</ol>

<p><img src="/images/skimmable-literature-images/figure1.png" alt="" /></p>

<p><strong>Source:</strong> Figure 1 (Section 4.2) Neural Networks for Joint Sentence Classification in Medical Paper Abstracts</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Setup token inputs/model
</span><span class="n">token_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">token_input</span><span class="sh">"</span><span class="p">)</span>
<span class="n">token_embeddings</span> <span class="o">=</span> <span class="nf">tf_hub_embedding_layer</span><span class="p">(</span><span class="n">token_inputs</span><span class="p">)</span>
<span class="n">token_output</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">token_embeddings</span><span class="p">)</span>
<span class="n">token_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">token_inputs</span><span class="p">,</span>
                             <span class="n">outputs</span><span class="o">=</span><span class="n">token_output</span><span class="p">)</span>

<span class="c1"># 2. Setup char inputs/models
</span><span class="n">char_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_input</span><span class="sh">"</span><span class="p">)</span>
<span class="n">char_vectors</span> <span class="o">=</span> <span class="nf">char_vectorizer</span><span class="p">(</span><span class="n">char_inputs</span><span class="p">)</span>
<span class="n">char_embeddings</span> <span class="o">=</span> <span class="nf">char_embed</span><span class="p">(</span><span class="n">char_vectors</span><span class="p">)</span>
<span class="n">char_bi_lstm</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">24</span><span class="p">))(</span><span class="n">char_embeddings</span><span class="p">)</span> <span class="c1"># bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf
</span><span class="n">char_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">char_inputs</span><span class="p">,</span>
                            <span class="n">outputs</span><span class="o">=</span><span class="n">char_bi_lstm</span><span class="p">)</span>

<span class="c1"># 3. Concatenate token and char inputs (create hybrid token embedding)
</span><span class="n">token_char_concat</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Concatenate</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">token_char_hybrid</span><span class="sh">"</span><span class="p">)([</span><span class="n">token_model</span><span class="p">.</span><span class="n">output</span><span class="p">,</span>
                                                                  <span class="n">char_model</span><span class="p">.</span><span class="n">output</span><span class="p">])</span> <span class="c1"># passing outputs of step 1 &amp; 2 and combining them
</span>
<span class="c1"># 4. Create output layers - adding in Dropout, discussed in section 4.2 of https://arxiv.org/pdf/1612.05251.pdf
</span><span class="n">combined_dropout</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">token_char_concat</span><span class="p">)</span> <span class="c1"># set half of input units to 0 which will help prevent overfitting
</span><span class="n">combined_dense</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">combined_dropout</span><span class="p">)</span>
<span class="n">final_dropout</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">combined_dense</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">final_dropout</span><span class="p">)</span>

<span class="c1"># 5. Construct model with char and token inputs
</span><span class="n">model_4</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">token_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">char_model</span><span class="p">.</span><span class="nb">input</span><span class="p">],</span>
                         <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">model_4_token_and_char_embeddings</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># get a summary of our model
</span><span class="n">model_4</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_4_token_and_char_embeddings"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 char_input (InputLayer)     [(None, 1)]                  0         []                            
                                                                                                  
 token_input (InputLayer)    [(None,)]                    0         []                            
                                                                                                  
 char_vectorizer (TextVecto  (None, 290)                  0         ['char_input[0][0]']          
 rization)                                                                                        
                                                                                                  
 universal_sentence_encoder  (None, 512)                  2567978   ['token_input[0][0]']         
  (KerasLayer)                                            24                                      
                                                                                                  
 char_embed (Embedding)      (None, 290, 25)              1425      ['char_vectorizer[1][0]']     
                                                                                                  
 dense_5 (Dense)             (None, 128)                  65664     ['universal_sentence_encoder[1
                                                                    ][0]']                        
                                                                                                  
 bidirectional (Bidirection  (None, 48)                   9600      ['char_embed[1][0]']          
 al)                                                                                              
                                                                                                  
 token_char_hybrid (Concate  (None, 176)                  0         ['dense_5[0][0]',             
 nate)                                                               'bidirectional[0][0]']       
                                                                                                  
 dropout (Dropout)           (None, 176)                  0         ['token_char_hybrid[0][0]']   
                                                                                                  
 dense_6 (Dense)             (None, 128)                  22656     ['dropout[0][0]']             
                                                                                                  
 dropout_1 (Dropout)         (None, 128)                  0         ['dense_6[0][0]']             
                                                                                                  
 dense_7 (Dense)             (None, 5)                    645       ['dropout_1[0][0]']           
                                                                                                  
==================================================================================================
Total params: 256897814 (979.99 MB)
Trainable params: 99990 (390.59 KB)
Non-trainable params: 256797824 (979.61 MB)
</code></pre></div></div>

<p>This is quite a big model, so let’s go ahead and visualize it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot hybrid token and character model
</span><span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="nf">plot_model</span><span class="p">(</span><span class="n">model_4</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/model-4.png" alt="" /></p>

<p>Now, we can see how our model concatenates the <code class="language-plaintext highlighter-rouge">token_model</code> and the <code class="language-plaintext highlighter-rouge">char_model</code>. Now, we need to combine our token and character datasets in order to feed into our model.</p>

<h3 id="combining-token-and-character-data-into-a-tfdata-dataset">Combining token and character data into a tf.data Dataset</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Combine chars and tokens into a dataset
</span><span class="n">train_char_token_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">train_sentences</span><span class="p">,</span> <span class="n">train_chars</span><span class="p">))</span> <span class="c1"># make data
</span><span class="n">train_char_token_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">train_labels_one_hot</span><span class="p">))</span> <span class="c1"># make labels
</span><span class="n">train_char_token_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">zip</span><span class="p">((</span><span class="n">train_char_token_data</span><span class="p">,</span> <span class="n">train_char_token_labels</span><span class="p">))</span> <span class="c1"># combine data and labels
</span>
<span class="c1"># Prefetch and batch train data
</span><span class="n">train_char_token_dataset</span> <span class="o">=</span> <span class="n">train_char_token_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Repeat the above steps for our validation data
</span><span class="n">val_char_token_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">val_sentences</span><span class="p">,</span> <span class="n">val_chars</span><span class="p">))</span> <span class="c1"># make data
</span><span class="n">val_char_token_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">val_labels_one_hot</span><span class="p">))</span> <span class="c1"># make labels
</span><span class="n">val_char_token_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">zip</span><span class="p">((</span><span class="n">val_char_token_data</span><span class="p">,</span> <span class="n">val_char_token_labels</span><span class="p">))</span> <span class="c1"># combine data and labels
</span>
<span class="c1"># Prefetch and batch train data
</span><span class="n">val_char_token_dataset</span> <span class="o">=</span> <span class="n">val_char_token_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Check out our training char and token embedding dataset
</span><span class="n">train_char_token_dataset</span><span class="p">,</span> <span class="n">val_char_token_dataset</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;,
 &lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;)
</code></pre></div></div>

<h3 id="fitting-a-model-on-token-and-character-level-sequences">Fitting a model on token and character-level sequences</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model on tokens and chars
</span><span class="n">history_model_4</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_char_token_dataset</span><span class="p">,</span>
                              <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_char_token_dataset</span><span class="p">)),</span>
                              <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                              <span class="n">validation_data</span><span class="o">=</span><span class="n">val_char_token_dataset</span><span class="p">,</span>
                              <span class="n">validation_steps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_char_token_dataset</span><span class="p">)))</span>

<span class="c1"># Evaluate on the whole validation dataset
</span><span class="n">model_4</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">val_char_token_dataset</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>945/945 [==============================] - 19s 20ms/step - loss: 0.6752 - accuracy: 0.7449
[0.6751742362976074, 0.744869589805603]
</code></pre></div></div>

<p>Seems like our accuracy did not improve by a lot, but let’s make some predictions so we can evaluate our model on the rest of the metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make predictions using the token-character model hybrid
</span><span class="n">model_4_pred_probs</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">val_char_token_dataset</span><span class="p">)</span>

<span class="c1"># Format pred probs into pred labels
</span><span class="n">model_4_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">model_4_pred_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_4_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 2])&gt;</code></p>

<h3 id="model-4-results">Model 4 results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get results of token-char-hybrid model
</span><span class="n">model_4_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="o">=</span><span class="n">model_4_preds</span><span class="p">)</span>
<span class="n">model_4_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 74.48695882430823,
 'precision': 74.39903884384483,
 'recall': 74.48695882430823,
 'f1': 74.24259393989037}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print model_3_results
</span><span class="n">model_3_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 72.72275916854231,
 'precision': 72.22725961774881,
 'recall': 72.72275916854231,
 'f1': 71.93359956231276}
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">model_4</code> was able to beat <code class="language-plaintext highlighter-rouge">model_3</code>, but only by a little bit. Maybe, we can try utilizing positional embeddings to improve our metrics.</p>

<h2 id="model-5-transfer-learning-with-pretrained-token-embeddings--character-embeddings--positional-embeddings">Model 5: Transfer learning with pretrained token embeddings + character embeddings + positional embeddings</h2>

<p>Positional embeddings in NLP add information about the order of words in a sentence, helping the model understand the sequence or arrangement of words, like giving each word a unique position number.</p>

<p>With this method, we can help our model recognise where the objective, backgrounds, methods, and results generally appear in an abstract.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/train-df.png" alt="" /></p>

<blockquote>
  <p><strong>Note:</strong> Any engineered features used to train a model need to be available at test time. In our case, line numbers and total lines are available.</p>
</blockquote>

<h3 id="create-positional-embeddings">Create positional embeddings</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># How many different line numbers are there?
</span><span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">line_number</span><span class="sh">"</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0     15000
1     15000
2     15000
3     15000
4     14992
5     14949
6     14758
7     14279
8     13346
9     11981
10    10041
11     7892
12     5853
13     4152
14     2835
15     1861
16     1188
17      751
18      462
19      286
20      162
21      101
22       66
23       33
24       22
25       14
26        7
27        4
28        3
29        1
30        1
Name: line_number, dtype: int64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use TensorFlow to create one-hot-encoded tensors of our "line_number" column
</span><span class="n">train_line_numbers_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">line_number</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">val_line_numbers_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">val_df</span><span class="p">[</span><span class="sh">"</span><span class="s">line_number</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">test_line_numbers_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">line_number</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">train_line_numbers_one_hot</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">train_line_numbers_one_hot</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;tf.Tensor: shape=(10, 15), dtype=float32, numpy=
 array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],
       dtype=float32)&gt;,
 TensorShape([180040, 15]))
</code></pre></div></div>
<p>Now we’ve encoded our line numbers feature, let’s do the same for our total lines feature.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use TensorFlow to create one-hot-encoded tensors of our "total_lines" column
</span><span class="n">train_total_lines_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">total_lines</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">val_total_lines_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">val_df</span><span class="p">[</span><span class="sh">"</span><span class="s">total_lines</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">test_total_lines_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">total_lines</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">(),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">train_total_lines_one_hot</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">train_line_numbers_one_hot</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;tf.Tensor: shape=(10, 20), dtype=float32, numpy=
 array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0.]], dtype=float32)&gt;,
 TensorShape([180040, 15]))
</code></pre></div></div>

<h3 id="building-a-tribrid-embedding-model">Building a tribrid embedding model</h3>

<ol>
  <li>Create a token-level model.</li>
  <li>Create a character-level model.</li>
  <li>Create a model for the “line_number” feature.</li>
  <li>Create a model for the “total_lines” feature.</li>
  <li>Combine the outputs of 1 &amp; 2 using tf.keras.layers.Concatenate.</li>
  <li>Combine the outputs of 3, 4, and 5 using tf.keras.layers.Concatenate.</li>
  <li>Create an output layer to accept the tribrid embedding and output label probabilities.</li>
  <li>Combine the inputs of 1, 2, 3, 4, and outputs of 7 into a tf.keras.Model.</li>
</ol>

<p>Before we create our model, let’s see the shapes of our input data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_line_numbers_one_hot</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_total_lines_one_hot</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_line_numbers_one_hot</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dtype</span><span class="p">,</span> <span class="n">train_total_lines_one_hot</span><span class="p">.</span><span class="n">dtype</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: (TensorShape([15]), TensorShape([20]), tf.float32, tf.float32)</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Token inputs
</span><span class="n">token_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">token_inputs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">token_embeddings</span> <span class="o">=</span> <span class="nf">tf_hub_embedding_layer</span><span class="p">(</span><span class="n">token_inputs</span><span class="p">)</span>
<span class="n">token_outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">token_embeddings</span><span class="p">)</span>
<span class="n">token_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">token_inputs</span><span class="p">,</span>
                             <span class="n">outputs</span><span class="o">=</span><span class="n">token_outputs</span><span class="p">)</span>

<span class="c1"># 2. Char inputs
</span><span class="n">char_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_inputs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">char_vectors</span> <span class="o">=</span> <span class="nf">char_vectorizer</span><span class="p">(</span><span class="n">char_inputs</span><span class="p">)</span>
<span class="n">char_embeddings</span> <span class="o">=</span> <span class="nf">char_embed</span><span class="p">(</span><span class="n">char_vectors</span><span class="p">)</span>
<span class="n">char_bi_lstm</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">24</span><span class="p">))(</span><span class="n">char_embeddings</span><span class="p">)</span>
<span class="n">char_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">char_inputs</span><span class="p">,</span>
                            <span class="n">outputs</span><span class="o">=</span><span class="n">char_bi_lstm</span><span class="p">)</span>

<span class="c1"># 3. Line numbers model
</span><span class="n">line_number_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">line_number_inputs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">line_number_inputs</span><span class="p">)</span>
<span class="n">line_number_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">line_number_inputs</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 4. Total lines model
</span><span class="n">total_lines_inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">total_lines_inputs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">total_lines_inputs</span><span class="p">)</span>
<span class="n">total_lines_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">total_lines_inputs</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># 5. Combine token and char embeddings into a hybrid embedding
</span><span class="n">combined_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Concatenate</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_token_hybrid_embedding</span><span class="sh">"</span><span class="p">)([</span><span class="n">token_model</span><span class="p">.</span><span class="n">output</span><span class="p">,</span>
                                                                              <span class="n">char_model</span><span class="p">.</span><span class="n">output</span><span class="p">])</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">combined_embeddings</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># 6. Combine positional embeddings with combined token and char embeddings
</span><span class="n">tribrid_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Concatenate</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">char_token_positional_embedding</span><span class="sh">"</span><span class="p">)([</span><span class="n">line_number_model</span><span class="p">.</span><span class="n">output</span><span class="p">,</span>
                                                                                 <span class="n">total_lines_model</span><span class="p">.</span><span class="n">output</span><span class="p">,</span>
                                                                                 <span class="n">z</span><span class="p">])</span>

<span class="c1"># 7. Create output layer
</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">output_layer</span><span class="sh">"</span><span class="p">)(</span><span class="n">tribrid_embeddings</span><span class="p">)</span>

<span class="c1"># 8. Put together a model with all kinds of inputs
</span><span class="n">model_5</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">line_number_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span>
                                 <span class="n">total_lines_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span>
                                 <span class="n">token_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span>
                                 <span class="n">char_model</span><span class="p">.</span><span class="nb">input</span><span class="p">],</span>
                         <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">model_5_tribrid_embedding_model</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Get a summary of our tribrid embedding model
</span><span class="n">model_5</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_5_tribrid_embedding_model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 char_inputs (InputLayer)    [(None, 1)]                  0         []                            
                                                                                                  
 token_inputs (InputLayer)   [(None,)]                    0         []                            
                                                                                                  
 char_vectorizer (TextVecto  (None, 290)                  0         ['char_inputs[0][0]']         
 rization)                                                                                        
                                                                                                  
 universal_sentence_encoder  (None, 512)                  2567978   ['token_inputs[0][0]']        
  (KerasLayer)                                            24                                      
                                                                                                  
 char_embed (Embedding)      (None, 290, 25)              1425      ['char_vectorizer[2][0]']     
                                                                                                  
 dense_8 (Dense)             (None, 128)                  65664     ['universal_sentence_encoder[2
                                                                    ][0]']                        
                                                                                                  
 bidirectional_1 (Bidirecti  (None, 48)                   9600      ['char_embed[2][0]']          
 onal)                                                                                            
                                                                                                  
 char_token_hybrid_embeddin  (None, 176)                  0         ['dense_8[0][0]',             
 g (Concatenate)                                                     'bidirectional_1[0][0]']     
                                                                                                  
 line_number_inputs (InputL  [(None, 15)]                 0         []                            
 ayer)                                                                                            
                                                                                                  
 total_lines_inputs (InputL  [(None, 20)]                 0         []                            
 ayer)                                                                                            
                                                                                                  
 dense_11 (Dense)            (None, 256)                  45312     ['char_token_hybrid_embedding[
                                                                    0][0]']                       
                                                                                                  
 dense_9 (Dense)             (None, 32)                   512       ['line_number_inputs[0][0]']  
                                                                                                  
 dense_10 (Dense)            (None, 32)                   672       ['total_lines_inputs[0][0]']  
                                                                                                  
 dropout_2 (Dropout)         (None, 256)                  0         ['dense_11[0][0]']            
                                                                                                  
 char_token_positional_embe  (None, 320)                  0         ['dense_9[0][0]',             
 dding (Concatenate)                                                 'dense_10[0][0]',            
                                                                     'dropout_2[0][0]']           
                                                                                                  
 output_layer (Dense)        (None, 5)                    1605      ['char_token_positional_embedd
                                                                    ing[0][0]']                   
                                                                                                  
==================================================================================================
Total params: 256922614 (980.08 MB)
Trainable params: 124790 (487.46 KB)
Non-trainable params: 256797824 (979.61 MB)
__________________________________________________________________________________________________
</code></pre></div></div>

<p>Let’s now plot <code class="language-plaintext highlighter-rouge">model_5</code> so we can visualize it since it is a huge model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot model_5 to explore it visually
</span><span class="nf">plot_model</span><span class="p">(</span><span class="n">model_5</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/model-5.png" alt="" /></p>

<p>We are now going to compile the model however, we’ll be utilizing label smoothing.</p>

<p>What is label smoothing?</p>

<p>If our model gets too confident on a single class (e.g. its prediction probability is really high), it may get stuck on that class and not consider other classes.</p>

<p>When our model is really confident about a label:
<code class="language-plaintext highlighter-rouge">[0.0, 0.0, 1.0, 0.0, 0.0]</code></p>

<p>What label smoothing does is it assigns some of the value from the highest pred prob to other classes, in turn, hopefully improving generalization: <code class="language-plaintext highlighter-rouge">[0.01, 0.01, 0.96, 0.01, 0.01]</code></p>

<p>With that being said, let’s go ahead and compile <code class="language-plaintext highlighter-rouge">model_5</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compile token, char, and positional embedding model
</span><span class="n">model_5</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">CategoricalCrossentropy</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span> <span class="c1"># helps to prevent overfitting
</span>                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="create-tribrid-embedding-datasets-using-tfdata">Create tribrid embedding datasets using tf.data</h3>

<p>Before we can fit our model, we need to alter our dataset in order to feed it into <code class="language-plaintext highlighter-rouge">model_5</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create training and validation datasets (with all four kinds of input data)
</span><span class="n">train_char_token_pos_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">train_line_numbers_one_hot</span><span class="p">,</span>
                                                                <span class="n">train_total_lines_one_hot</span><span class="p">,</span>
                                                                <span class="n">train_sentences</span><span class="p">,</span>
                                                                <span class="n">train_chars</span><span class="p">))</span> <span class="c1"># create features
</span>
<span class="n">train_char_token_pos_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">(</span><span class="n">train_labels_one_hot</span><span class="p">)</span> <span class="c1"># create labels
</span><span class="n">train_char_token_pos_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">zip</span><span class="p">((</span><span class="n">train_char_token_pos_data</span><span class="p">,</span> <span class="n">train_char_token_pos_labels</span><span class="p">))</span> <span class="c1"># put featurs &amp; labels together
</span>
<span class="c1"># Batch and prefetch dataset
</span><span class="n">train_char_token_pos_dataset</span> <span class="o">=</span> <span class="n">train_char_token_pos_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Do the same above but for the validation dataset
</span><span class="n">val_char_token_pos_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">val_line_numbers_one_hot</span><span class="p">,</span>
                                                              <span class="n">val_total_lines_one_hot</span><span class="p">,</span>
                                                              <span class="n">val_sentences</span><span class="p">,</span>
                                                              <span class="n">val_chars</span><span class="p">))</span> <span class="c1"># create features
</span>
<span class="n">val_char_token_pos_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">(</span><span class="n">val_labels_one_hot</span><span class="p">)</span> <span class="c1"># create labels
</span><span class="n">val_char_token_pos_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">zip</span><span class="p">((</span><span class="n">val_char_token_pos_data</span><span class="p">,</span> <span class="n">val_char_token_pos_labels</span><span class="p">))</span> <span class="c1"># put featurs &amp; labels together
</span>
<span class="c1"># Batch and prefetch dataset
</span><span class="n">val_char_token_pos_dataset</span> <span class="o">=</span> <span class="n">val_char_token_pos_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="n">train_char_token_pos_dataset</span><span class="p">,</span> <span class="n">val_char_token_pos_dataset</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;,
 &lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;)
</code></pre></div></div>

<h3 id="fitting-evaluating-and-making-predictions-with-our-tribrid-model">Fitting, evaluating, and making predictions with our tribrid model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit our tribrid embedding model
</span><span class="n">history_model_5</span> <span class="o">=</span> <span class="n">model_5</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_char_token_pos_dataset</span><span class="p">,</span>
                              <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_char_token_pos_dataset</span><span class="p">)),</span>
                              <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                              <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">validation_data</span><span class="o">=</span><span class="n">val_char_token_pos_dataset</span><span class="p">,</span>
                              <span class="n">validation_steps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_char_token_pos_dataset</span><span class="p">)))</span>

<span class="c1"># Make predictions with the char token pos model
</span><span class="n">model_5_pred_probs</span> <span class="o">=</span> <span class="n">model_5</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">val_char_token_pos_dataset</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Convert pred probs to pred labels
</span><span class="n">model_5_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">model_5_pred_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_5_preds</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])&gt;</code></p>

<h3 id="model-5-results">Model 5 results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate results of char token pos model
</span><span class="n">model_5_results</span> <span class="o">=</span> <span class="nf">calculate_results</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">val_labels_encoded</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="o">=</span><span class="n">model_5_preds</span><span class="p">)</span>
<span class="n">model_5_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 86.29683569442605,
 'precision': 86.4369522985925,
 'recall': 86.29683569442605,
 'f1': 85.99966773630177}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print model_4_results
</span><span class="n">model_4_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'accuracy': 74.48695882430823,
 'precision': 74.39903884384483,
 'recall': 74.48695882430823,
 'f1': 74.24259393989037}
</code></pre></div></div>

<p>Woah! Would you look at that. <code class="language-plaintext highlighter-rouge">model_5</code> is substantially better than the <code class="language-plaintext highlighter-rouge">model_4</code>. It’s more accurate (accuracy increased from 74.48% to 86.29%), makes fewer mistakes (precision increased from 74.39% to 86.43%), and misses less (recall increased from 74.48% to 86.29%). Overall, it’s just a lot more effective (F1 score increased from 74.24% to 85.99%).</p>

<p>Let’s now compare all our experiments with each other.</p>

<h2 id="compare-model-results">Compare model results</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Combine model results into a dataframe
</span><span class="n">all_model_results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">model_0_baseline</span><span class="sh">"</span><span class="p">:</span> <span class="n">baseline_results</span><span class="p">,</span>
                                  <span class="sh">"</span><span class="s">model_1_custom_token_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_1_results</span><span class="p">,</span>
                                  <span class="sh">"</span><span class="s">model_2_pretrained_token_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_2_results</span><span class="p">,</span>
                                  <span class="sh">"</span><span class="s">model_3_custom_char_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_3_results</span><span class="p">,</span>
                                  <span class="sh">"</span><span class="s">model_4_hybrid_char_token_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_4_results</span><span class="p">,</span>
                                  <span class="sh">"</span><span class="s">model_5_pos_char_token_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_5_results</span><span class="p">})</span>

<span class="n">all_model_results</span> <span class="o">=</span> <span class="n">all_model_results</span><span class="p">.</span><span class="nf">transpose</span><span class="p">()</span>
<span class="n">all_model_results</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/results-df.png" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sort models results by f1-score
</span><span class="n">all_model_results</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">"</span><span class="s">f1</span><span class="sh">"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="sh">"</span><span class="s">f1</span><span class="sh">"</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="sh">"</span><span class="s">bar</span><span class="sh">"</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">));</span>
</code></pre></div></div>
<p>Let’s graph our results by f1-score to visualize them.
<code class="language-plaintext highlighter-rouge">Output: </code></p>

<p><img src="/images/skimmable-literature-images/results-graph.png" alt="" /></p>

<p>So, we can clearly see that <code class="language-plaintext highlighter-rouge">model_5_pos_char_token_embedding</code> performed the best out of all our models.</p>

<h2 id="conclusion">Conclusion</h2>

<p>After a few experiments, I am satisfied with the results of <code class="language-plaintext highlighter-rouge">model_5_pos_char_token_embedding</code>. Maybe in the future, I can try further improve the metrics, but as of right now, I am satisfied with these results.</p>

<h2 id="the-full-code">The Full Code</h2>

<p>You can check out all the code together on my <a href="https://github.com/samikamal21/skimmable-literature">Skimmable-Literature repository.</a></p>

        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Sami Kamal. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
